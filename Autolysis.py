# -*- coding: utf-8 -*-
"""Autolysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SadXDnfNP1VuUDg2-3u6EbTVuvJfF1Ur
"""

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import requests
import json
import numpy as np
from tabulate import tabulate
from scipy import stats
from datetime import datetime

# API proxy configuration
api_proxy_token = "eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIzZjEwMDI5NjdAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.9g2oohxew3yz4yPI3Q_j0ic_ZiZn_qPJA6rajoiEs9Q"
api_proxy_base_url = "https://aiproxy.sanand.workers.dev/openai/v1"

def upload_files():
    """Prompt the user to upload files."""
    print("Please upload your CSV files.")
    uploaded = files.upload()
    return list(uploaded.keys())

def read_csv(filename):
    """Read the CSV file and return a DataFrame."""
    try:
        df = pd.read_csv(filename, encoding="utf-8")
        print(f"Dataset loaded: {filename}")
        return df
    except UnicodeDecodeError:
        print(f"Encoding issue detected with {filename}. Trying 'latin1'.")
        return pd.read_csv(filename, encoding="latin1")
    except Exception as e:
        print(f"Error loading {filename}: {e}")
        return None

def analyze_data(df):
    """Perform basic analysis on the dataset."""
    analysis = {
        "shape": df.shape,
        "columns": df.columns.tolist(),
        "missing_values": df.isnull().sum().to_dict(),
        "summary_statistics": df.describe(include="all").to_dict()
    }
    return analysis

def visualize_data(df, output_prefix, output_dir):
    """Generate visualizations for the dataset."""
    charts = []
    ensure_directory_exists(output_dir)

    # Example 1: Correlation Heatmap (if numeric data exists)
    numeric_columns = df.select_dtypes(include=["number"]).columns
    if len(numeric_columns) > 0:
        plt.figure(figsize=(14, 12))
        heatmap = sns.heatmap(
            df[numeric_columns].corr(),
            annot=True,
            cmap="coolwarm",
            fmt=".2f",
            cbar_kws={'shrink': 0.8}
        )
        heatmap.set_title("Correlation Heatmap", fontsize=16, pad=20)
        plt.tight_layout()
        heatmap_file = os.path.join(output_dir, f"{output_prefix}_heatmap.png")
        plt.savefig(heatmap_file, dpi=300)
        charts.append(heatmap_file)
        plt.close()

    # Example 2: Bar Plot for the first categorical column
    categorical_columns = df.select_dtypes(include=["object"]).columns
    if len(categorical_columns) > 0:
        plt.figure(figsize=(14, 8))
        top_categories = df[categorical_columns[0]].value_counts().head(10)
        top_categories.sort_values().plot(kind="barh", color="skyblue")
        plt.title(f"Top 10 {categorical_columns[0]} Categories", fontsize=16)
        plt.tight_layout()
        barplot_file = os.path.join(output_dir, f"{output_prefix}_barplot.png")
        plt.savefig(barplot_file, dpi=300)
        charts.append(barplot_file)
        plt.close()

    if len(numeric_columns) > 0:
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df[numeric_columns], orient="h", palette="Set2")
        plt.title("Box Plot for Numeric Columns")
        boxplot_file = os.path.join(output_dir, f"{output_prefix}_boxplot.png")
        plt.savefig(boxplot_file, dpi=300)
        charts.append(boxplot_file)
        plt.close()


    return charts

def narrate_story(analysis, charts, filename):
    """Use GPT-4o-Mini to narrate a story about the analysis."""
    summary_prompt = f"""
    I analyzed a dataset from {filename}. It has the following details:
    - Shape: {analysis['shape']}
    - Columns: {analysis['columns']}
    - Missing Values: {analysis['missing_values']}
    - Summary Statistics: {analysis['summary_statistics']}

    Write a short summary of the dataset, key insights, and recommendations. Refer to the charts where necessary.
    """
    url = f"{api_proxy_base_url}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_proxy_token}"
    }
    data = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": summary_prompt}],
        "temperature": 0.7
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        return f"Story generation failed: {e}"

def save_markdown(story, charts, output_file):
    """Save the narrated story and chart references to a README.md file."""
    with open(output_file, "w") as f:
        f.write("# Analysis Report\n\n")
        f.write(story + "\n\n")
        for chart in charts:
            f.write(f"![Chart](./{os.path.basename(chart)})\n")

def save_table(df, output_file):
    """Save the dataframe as a formatted table in the README.md file."""
    table = tabulate(df.head(), headers='keys', tablefmt='pipe', showindex=False)
    with open(output_file, "a") as f:
        f.write("\n## Sample Data\n\n")
        f.write(table + "\n")

def main():
    # Define subdirectories for outputs
    subdirs = ["goodreads", "happiness", "media"]
    for subdir in subdirs:
        ensure_directory_exists(subdir)

    # Automatically process all CSV files in the folder
    csv_files = [f for f in os.listdir() if f.endswith('.csv')]

    if not csv_files:
        print("No CSV files found in the directory.")
        return

    for filename in csv_files:
        print(f"Processing {filename}...")

        # Determine output folder based on filename
        if "goodreads" in filename.lower():
            output_dir = "goodreads"
        elif "happiness" in filename.lower():
            output_dir = "happiness"
        elif "media" in filename.lower():
            output_dir = "media"
        else:
            output_dir = "miscellaneous"

        ensure_directory_exists(output_dir)

        # Load dataset
        df = read_csv(filename)

        # Analyze dataset
        analysis = analyze_data(df)

        # Visualize data
        output_prefix = os.path.splitext(filename)[0]
        charts = visualize_data(df, output_prefix, output_dir)

        # Narrate story
        story = narrate_story(analysis, charts, filename)

        # Save README.md
        readme_file = os.path.join(output_dir, "README.md")
        save_markdown(story, charts, readme_file)

        # Save Sample Data Table
        save_table(df, readme_file)

        print(f"Analysis completed for {filename}. Outputs saved in {output_dir}.")

if __name__ == "__main__":
    main()