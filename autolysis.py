# -*- coding: utf-8 -*-
"""autolysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_jfxPf0nGAkKiqnXSqcwMahXOXM60-kk
"""

import os
import sys
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json
from google.colab import files
import chardet

# Print starting message
print("Starting analysis script")

# Verify Environment Variable
if "AIPROXY_TOKEN" not in os.environ:
    print("Error: AIPROXY_TOKEN environment variable is not set.")
    sys.exit(1)

AIPROXY_TOKEN = os.environ["AIPROXY_TOKEN"]

# Function to detect file encoding
def detect_encoding(file_path):
    """Detect the file encoding using chardet."""
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

# Function to load dataset with encoding handling
def load_dataset(file_path):
    try:
        # Try loading with 'utf-8' encoding first
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        # If 'utf-8' fails, try using detected encoding
        encoding = detect_encoding(file_path)
        print(f"UTF-8 encoding failed. Trying {encoding} encoding.")
        df = pd.read_csv(file_path, encoding=encoding)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        sys.exit(1)
    return df

# Function to analyze the dataset
def analyze_dataset(df):
    summary = {
        "columns": list(df.columns),
        "data_types": df.dtypes.to_dict(),
        "missing_values": df.isnull().sum().to_dict(),
        "summary_stats": df.describe(include="all").to_dict()
    }
    return summary

# Function to visualize data
def visualize_data(df, output_folder):
    # Select only numeric columns for correlation
    numeric_df = df.select_dtypes(include=['number'])

    # Example: Correlation heatmap
    if not numeric_df.empty:
        plt.figure(figsize=(10, 8))
        sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm")
        plt.title("Correlation Heatmap")
        heatmap_path = os.path.join(output_folder, "heatmap.png")
        plt.savefig(heatmap_path)
        plt.close()
        return [heatmap_path]
    else:
        print("No numeric data available for correlation analysis.")
        return []

# Function to send request to AI Proxy (GPT-4o-Mini)
def generate_narration(prompt):
    url = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {AIPROXY_TOKEN}",
    }
    data = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": prompt}],
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error generating narration: {e}")
        sys.exit(1)

# Main function
def main():
    print("Upload your CSV file using the file upload dialog.")
    uploaded = files.upload()
    file_path = next(iter(uploaded.keys()))  # Get the first uploaded file

    # Set output folder name based on the file name (without extension)
    output_folder = os.path.splitext(file_path)[0]
    os.makedirs(output_folder, exist_ok=True)

    # Load dataset
    df = load_dataset(file_path)

    # Analyze dataset
    summary = analyze_dataset(df)

    # Visualize data
    chart_paths = visualize_data(df, output_folder)

    # Generate narration
    prompt = f"""
    I analyzed the dataset '{file_path}' with the following characteristics:
    {summary}.
    Based on the analysis, here is the narrative:
    """
    story = generate_narration(prompt)

    # Write to README.md
    readme_path = os.path.join(output_folder, "README.md")
    with open(readme_path, "w") as f:
        f.write(f"# Automated Analysis Report\n\n## Summary\n{summary}\n\n")
        f.write(f"## Story\n{story}\n\n")
        f.write("## Visualizations\n")
        for chart in chart_paths:
            f.write(f"![Visualization]({chart})\n")

    print(f"Analysis complete. Results saved in {output_folder}")
    print(f"Download the results from the {output_folder} folder.")
    # Optionally, allow user to download the results
    for file in os.listdir(output_folder):
        files.download(os.path.join(output_folder, file))

if __name__ == "__main__":
    main()

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import requests

# Function to load dataset
def load_dataset(file_path):
    try:
        # Try reading the dataset with ISO-8859-1 encoding as fallback
        return pd.read_csv(file_path, encoding="ISO-8859-1")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        raise

# Function to analyze the dataset
def analyze_dataset(df):
    summary = {
        "columns": list(df.columns),
        "data_types": df.dtypes.to_dict(),
        "missing_values": df.isnull().sum().to_dict(),
        "summary_stats": df.describe(include="all").to_dict()
    }
    return summary

# Function to visualize data
def visualize_data(df, output_folder):
    numeric_df = df.select_dtypes(include=['number'])
    chart_paths = []

    if not numeric_df.empty:
        plt.figure(figsize=(10, 8))
        sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm")
        plt.title("Correlation Heatmap")
        heatmap_path = os.path.join(output_folder, "heatmap.png")
        plt.savefig(heatmap_path)
        plt.close()
        chart_paths.append(heatmap_path)

    return chart_paths

# Function to send request to AI Proxy (GPT-4o-Mini)
def generate_narration(prompt):
    url = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {AIPROXY_TOKEN}",
    }
    data = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": prompt}],
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error generating narration: {e}")
        raise

# Main function
def main():
    # Upload the CSV file in Colab environment
    uploaded = files.upload()
    file_path = next(iter(uploaded.keys()))  # Get the uploaded file path

    # Create an output folder from the dataset's name (removing file extension and replacing spaces with underscores)
    output_folder = file_path.split('.')[0].replace(' ', '_')
    os.makedirs(output_folder, exist_ok=True)

    # Load dataset
    try:
        df = load_dataset(file_path)
    except Exception as e:
        print(f"Dataset loading failed: {e}")
        return

    # Analyze dataset
    summary = analyze_dataset(df)

    # Visualize data and save the charts
    chart_paths = visualize_data(df, output_folder)

    # Generate the LLM narration based on the analysis
    prompt = f"Analyze the following dataset: {summary}. Provide a detailed story with insights and implications."
    story = generate_narration(prompt)

    # Create a README.md file in the output folder
    readme_path = os.path.join(output_folder, "README.md")
    with open(readme_path, "w") as f:
        f.write(f"# Automated Analysis Report\n\n## Summary\n{summary}\n\n")
        f.write(f"## Story\n{story}\n\n")
        f.write("## Visualizations\n")
        for chart in chart_paths:
            f.write(f"![Visualization]({chart})\n")

    print(f"Analysis complete. Results saved in {output_folder}")

if __name__ == "__main__":
    main()